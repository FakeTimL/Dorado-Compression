[model]
type = "seqdistmodel"
package = "bonito.transformer"

[scaling]
strategy = "pa"

[run_info]
sample_type = "dna"
sample_rate = 5000

[standardisation]
standardise = 1
mean = 93.69239463939118
stdev = 23.506745239082388

[qscore]
scale = 1.05
bias = 1.3

[basecaller]
batchsize = 128
chunksize = 12000
overlap = 600

[training]
command = "train"
training_directory = "sparse_0.578125_retrained"
config = "PosixPath('/vol/bitbucket/bl1821/bonito_env/lib/python3.10/site-packages/bonito/models/configs/dna_r9.4.1@v3.1.toml')"
pretrained = "sparse_0.578125"
directory = "PosixPath('../models/train_data/je')"
device = "cuda"
lr = "2e-3"
seed = 25
epochs = 5
batch = 32
chunks = 200
valid_chunks = 50
no_amp = false
force = false
restore_optim = false
nondeterministic = false
save_optim_every = 10
grad_accum_split = 1
quantile_grad_clip = true
num_workers = 4
func = "<function main at 0x70a9980cd000>"
pwd = "/vol/bitbucket/bl1821/frankenstein/prune"

[model.seqdist]
state_len = 5
alphabet = [ "N", "A", "C", "G", "T",]

[model.encoder]
type = "namedserial"

[model.encoder.conv]
type = "serial"
[[model.encoder.conv.sublayers]]
type = "convolution"
insize = 1
size = 64
bias = true
winlen = 5
stride = 1
padding = 2
activation = "swish"
norm = "batchnorm"

[[model.encoder.conv.sublayers]]
type = "convolution"
insize = 64
size = 64
bias = true
winlen = 5
stride = 1
padding = 2
activation = "swish"
norm = "batchnorm"

[[model.encoder.conv.sublayers]]
type = "convolution"
insize = 64
size = 128
bias = true
winlen = 9
stride = 3
padding = 4
activation = "swish"
norm = "batchnorm"

[[model.encoder.conv.sublayers]]
type = "convolution"
insize = 128
size = 128
bias = true
winlen = 9
stride = 2
padding = 4
activation = "swish"
norm = "batchnorm"

[[model.encoder.conv.sublayers]]
type = "convolution"
insize = 128
size = 512
bias = true
winlen = 5
stride = 2
padding = 2
activation = "swish"
norm = "batchnorm"

[[model.encoder.conv.sublayers]]
type = "permute"
dims = [ 0, 2, 1,]

[model.encoder.transformer_encoder]
type = "stack"
depth = 18

[model.encoder.upsample]
type = "linearupsample"
d_model = 512
scale_factor = 2

[model.encoder.crf]
type = "linearcrfencoder"
insize = 512
n_base = 4
state_len = 5
bias = false
scale = 5.0
blank_score = 2.0
expand_blanks = true
permute = [ 1, 0, 2,]

[model.encoder.transformer_encoder.layer]
type = "transformerencoderlayer"
d_model = 512
nhead = 8
dim_feedforward = 2048
deepnorm_alpha = 2.4494897
deepnorm_beta = 0.2886751
attn_window = [ 127, 128,]
